import asyncio
import logging
import sys
import os
from flask import Flask, request
from apscheduler.schedulers.background import BackgroundScheduler
from sqlalchemy import create_engine

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ä»–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import DB_FILE, DATABASE_URL, TRAIN_SECRET_KEY
from database import init_db
from api_client import fetch_all_data_concurrently
from analyzer import analyze_and_detect_signals
from ml_model import train_model
from trader import execute_trade_logic
# âœ… ä¿®æ­£ç‚¹: çµ±ä¸€ã•ã‚ŒãŸé€šçŸ¥é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from notifier import format_and_send_notification

# --- ãƒ­ã‚¬ãƒ¼è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s', stream=sys.stdout)

# --- Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ– ---
app = Flask(__name__)

# --- ç›£è¦–å¯¾è±¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ ---
TARGET_PAIRS = {
    'ethereum': ['0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640'], 'solana': ['EKpQGSJtjMFqKZ9KQanSqYXRcF8fBopzL7GMKjeRMpzu'],
    'base': ['0x42f2a4866f2bb6f272a81381dfc1a48053b98619'], 'bsc': ['0x58f876857a02d6762e0101bb5c46a8c1ed44dc16'],
    'arbitrum': ['0xc31e54c7a869b9fcbecc14363cf510d1c41fa441'], 'optimism': ['0x85149247691df622eac1a890620f5c276d48e269'],
    'polygon': ['0xa374094527e1673a86de625aa59517c5de346d32'], 'avalanche': ['0xf4003f4efbe8691862452f05301293b06024b46f']
}

# --- éåŒæœŸã‚¸ãƒ§ãƒ–ã®å®šç¾© ---
async def data_collection_job():
    logging.info("ğŸ”¬ Starting data collection job...")
    try:
        # ãƒ‡ãƒ¼ã‚¿åé›†è‡ªä½“ã¯api_clientã§è¡Œã„ã€çµæœã¯DBã«ä¿å­˜ã•ã‚Œã‚‹
        await fetch_all_data_concurrently(TARGET_PAIRS)
    except Exception as e:
        logging.critical(f"Error in data collection job: {e}", exc_info=True)

async def analysis_and_alert_job():
    """åˆ†æã€é€šçŸ¥ã€å–å¼•å®Ÿè¡Œã‚’çµ±æ‹¬ã™ã‚‹éåŒæœŸã‚¸ãƒ§ãƒ–"""
    logging.info("ğŸ“¡ Starting analysis and alert job...")
    try:
        all_data = await fetch_all_data_concurrently(TARGET_PAIRS)
        if all_data:
            # DBã‚¨ãƒ³ã‚¸ãƒ³ã‚’ç›´æ¥ä½¿ç”¨
            engine = create_engine(DATABASE_URL or f"sqlite:///{DB_FILE}")
            with engine.connect() as db_conn:
                with db_conn.begin():
                    longs, shorts, all_indicators, overview = analyze_and_detect_signals(all_data, db_conn)
                
                # å–å¼•ãƒ­ã‚¸ãƒƒã‚¯ã¯è‡ªèº«ã®é€šçŸ¥æ©Ÿèƒ½ã‚’æŒã¤
                execute_trade_logic(longs, shorts, all_indicators)

                # ã‚·ã‚°ãƒŠãƒ«é€šçŸ¥ï¼ˆå–å¼•ã¨ã¯åˆ¥ï¼‰
                notification_longs = longs[:3] if longs else []
                notification_shorts = shorts[:3] if shorts else []
                if notification_longs or notification_shorts:
                    # âœ… ä¿®æ­£ç‚¹: çµ±ä¸€ã•ã‚ŒãŸé–¢æ•°ã‚’æ­£ã—ãå‘¼ã³å‡ºã™
                    await format_and_send_notification(
                        (notification_longs, notification_shorts, overview), 
                        notification_type='signal'
                    )
                else:
                    logging.info("No significant signals found to notify.")
    except Exception as e:
        logging.critical(f"Error in analysis and alert job: {e}", exc_info=True)

# --- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç”¨ã®åŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ ---
def run_async_job(job_func):
    try:
        asyncio.run(job_func())
    except RuntimeError:
        # æ—¢ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œä¸­ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.create_task(job_func())
        else:
             asyncio.run(job_func())

def data_collection_wrapper(): run_async_job(data_collection_job)
def analysis_and_alert_wrapper(): run_async_job(analysis_and_alert_job)

# --- Webã‚µãƒ¼ãƒãƒ¼ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
@app.route('/')
def home():
    return "âœ… Trend Sentinel is running."

@app.route('/train')
def trigger_training():
    secret_key = request.args.get('secret')
    if not TRAIN_SECRET_KEY or secret_key != TRAIN_SECRET_KEY:
        return "ğŸš« Unauthorized", 401
    try:
        logging.info("Manual training triggered via webhook.")
        train_model()
        return "âœ… Training process started successfully.", 200
    except Exception as e:
        error_message = str(e)
        logging.error(f"Failed to start training: {error_message}")
        return f"ğŸ”¥ Error starting training: {error_message}", 500

# --- ãƒ¡ã‚¤ãƒ³ã®èµ·å‹•å‡¦ç† ---
def start_scheduler():
    init_db()
    scheduler = BackgroundScheduler(timezone="Asia/Tokyo")
    scheduler.add_job(data_collection_wrapper, 'cron', minute='*/15')
    scheduler.add_job(analysis_and_alert_wrapper, 'cron', hour='2,8,14,20', minute=2)
    scheduler.start()
    logging.info("Scheduler started in background.")
    # èµ·å‹•æ™‚ã®åˆå›ã‚¸ãƒ§ãƒ–
    data_collection_wrapper()

if __name__ == '__main__':
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ 'train' ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã®å‡¦ç†
    if len(sys.argv) > 1 and sys.argv[1].lower() == 'train':
        init_db()
        train_model()
    else:
        # é€šå¸¸ã®Webã‚µãƒ¼ãƒãƒ¼ãƒ¢ãƒ¼ãƒ‰
        start_scheduler()
        port = int(os.environ.get("PORT", 8080))
        app.run(host="0.0.0.0", port=port)
else:
    # Gunicornã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸå ´åˆã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’èµ·å‹•
    start_scheduler()
