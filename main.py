import asyncio
import logging
import sqlite3
import sys
import os
from flask import Flask
from apscheduler.schedulers.background import BackgroundScheduler

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ä»–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import DB_FILE
from database import init_db, insert_market_data_batch, update_future_growth_labels
from api_client import fetch_all_data_concurrently
from analyzer import analyze_and_detect_signals
from notifier import format_and_send_telegram_notification
from ml_model import train_model

# --- ãƒ­ã‚¬ãƒ¼è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s', stream=sys.stdout)

# --- Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ– ---
app = Flask(__name__)

# --- ç›£è¦–å¯¾è±¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ ---
TARGET_PAIRS = {
    'ethereum': ['0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640'],
    'solana': ['EKpQGSJtjMFqKZ9KQanSqYXRcF8fBopzL7GMKjeRMpzu'],
    'base': ['0x42f2a4866f2bb6f272a81381dfc1a48053b98619'],
    'bsc': ['0x58f876857a02d6762e0101bb5c46a8c1ed44dc16'],
    'arbitrum': ['0xc31e54c7a869b9fcbecc14363cf510d1c41fa441'],
    'optimism': ['0x85149247691df622eac1a890620f5c276d48e269'],
    'polygon': ['0xa374094527e1673a86de625aa59517c5de346d32'],
    'avalanche': ['0xf4003f4efbe8691862452f05301293b06024b46f']
}

# --- éåŒæœŸã‚¸ãƒ§ãƒ–ã®å®šç¾© ---
async def data_collection_job():
    """MLç”¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ãƒ»æ•´å½¢ã™ã‚‹éåŒæœŸã‚¸ãƒ§ãƒ–"""
    logging.info("ğŸ”¬ Starting data collection job...")
    try:
        all_data = await fetch_all_data_concurrently(TARGET_PAIRS)
        if all_data:
            with sqlite3.connect(DB_FILE, timeout=10) as db_conn:
                update_future_growth_labels(db_conn, all_data)
                insert_market_data_batch(db_conn, all_data)
    except Exception as e:
        logging.critical(f"Error in data collection job: {e}", exc_info=True)

async def analysis_and_alert_job():
    """åˆ†æã¨ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ã‚’è¡Œã†éåŒæœŸã‚¸ãƒ§-ãƒ–"""
    logging.info("ğŸ“¡ Starting analysis and alert job...")
    try:
        all_data = await fetch_all_data_concurrently(TARGET_PAIRS)
        if all_data:
            with sqlite3.connect(DB_FILE, timeout=10) as db_conn:
                longs, shorts, overview = analyze_and_detect_signals(all_data, db_conn)
                if longs or shorts:
                    await format_and_send_telegram_notification(longs, shorts, overview)
                else:
                    logging.info("No significant signals found to notify.")
    except Exception as e:
        logging.critical(f"Error in analysis and alert job: {e}", exc_info=True)

# --- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç”¨ã®åŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•° ---
def run_async_job(job_func):
    """éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # æ—¢ã«å®Ÿè¡Œä¸­ã®ãƒ«ãƒ¼ãƒ—ãŒã‚ã‚Œã°ã€ã‚¿ã‚¹ã‚¯ã¨ã—ã¦æŠ•å…¥
            asyncio.ensure_future(job_func())
        else:
            # ãƒ«ãƒ¼ãƒ—ãŒãªã‘ã‚Œã°ã€æ–°è¦ã«å®Ÿè¡Œ
            asyncio.run(job_func())
    except RuntimeError:
        # 'RuntimeError: Cannot run the event loop while another loop is running' å¯¾ç­–
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(job_func())

def data_collection_wrapper():
    run_async_job(data_collection_job)

def analysis_and_alert_wrapper():
    run_async_job(analysis_and_alert_job)

# --- Webã‚µãƒ¼ãƒãƒ¼ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
@app.route('/')
def home():
    """Renderã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return "âœ… Trend Sentinel is running."

# --- ãƒ¡ã‚¤ãƒ³ã®èµ·å‹•å‡¦ç† ---
def start_scheduler():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®åˆæœŸåŒ–ã¨ã‚¸ãƒ§ãƒ–ã®ç™»éŒ²"""
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–
    init_db()
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å‹•ä½œã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨
    scheduler = BackgroundScheduler(timezone="Asia/Tokyo")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç™»éŒ²
    # æ¯æ™‚1åˆ†ã«ãƒ‡ãƒ¼ã‚¿åé›†ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
    scheduler.add_job(data_collection_wrapper, 'cron', minute=1)
    # JST 02:02, 08:02, 14:02, 20:02ã«åˆ†æã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
    scheduler.add_job(analysis_and_alert_wrapper, 'cron', hour='2,8,14,20', minute=2)
    
    scheduler.start()
    logging.info("Scheduler started in background.")

    # èµ·å‹•æ™‚ã«åˆå›ã‚¸ãƒ§ãƒ–ã‚’æ‰‹å‹•ã§å®Ÿè¡Œ
    logging.info("Running initial jobs immediately after startup...")
    data_collection_wrapper()
    analysis_and_alert_wrapper()


# ã“ã®ifãƒ–ãƒ­ãƒƒã‚¯ã¯ã€Gunicornã‹ã‚‰èµ·å‹•ã•ã‚Œã‚‹éš›ã«é‡è¦
if __name__ == '__main__':
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã®åˆ¤å®š
    if len(sys.argv) > 1 and sys.argv[1].lower() == 'train':
        print("Running in training mode...")
        init_db() # å­¦ç¿’å‰ã«ã‚‚DBã‚’ç¢ºå®Ÿã«åˆæœŸåŒ–
        train_model()
    else:
        # é€šå¸¸ã®Webã‚µãƒ¼ãƒãƒ¼ãƒ¢ãƒ¼ãƒ‰
        start_scheduler()
        # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        # Renderç’°å¢ƒã§ã¯GunicornãŒappã‚’èµ·å‹•ã™ã‚‹ãŸã‚ã€ã“ã®runã¯å®Ÿè¡Œã•ã‚Œãªã„
        port = int(os.environ.get("PORT", 8080))
        app.run(host="0.0.0.0", port=port)
else:
    # Gunicornã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸå ´åˆã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’èµ·å‹•
    start_scheduler()